# ğŸ¤– Chatbot com LlamaIndex, LangChain, FastAPI e Streamlit

Este projeto Ã© um chatbot que utiliza **modelos de linguagem local via Ollama**, combinando **LlamaIndex** para indexaÃ§Ã£o de documentos e **LangChain** para orquestraÃ§Ã£o da resposta com base no conteÃºdo de arquivos PDF, TXT, DOCX e CSV.

---

## ğŸ“¦ Tecnologias Utilizadas

- [LlamaIndex](https://www.llamaindex.ai/)
- [LangChain](https://www.langchain.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Streamlit](https://streamlit.io/)
- [Ollama](https://ollama.com/)
- Embeddings: [`sentence-transformers/LaBSE`](https://huggingface.co/sentence-transformers/LaBSE)

---

## ğŸ“ Estrutura do Projeto

â”œâ”€â”€ indexador.py # Indexa documentos no diretÃ³rio data/

â”œâ”€â”€ main.py # API FastAPI com recuperaÃ§Ã£o e resposta via LLM

â”œâ”€â”€ app.py # Interface front-end com Streamlit

â”œâ”€â”€ storage_labse/ # Armazena Ã­ndice vetorial persistente

â”œâ”€â”€ data/ # Coloque aqui seus arquivos PDF, TXT, CSV, DOCX


---

## âš™ï¸ PrÃ©-requisitos

Antes de tudo, instale:

- Python 3.10 ou superior
- [Ollama](https://ollama.com/download) instalado e configurado

---

## ğŸ“¥ InstalaÃ§Ã£o

1. Clone o projeto:

bash
git clone https://github.com/seu-usuario/seu-repo-chatbot.git
cd seu-repo-chatbot

2. Crie um ambiente virtual:

python -m venv venv

source venv/bin/activate  # Linux/macOS

venv\Scripts\activate     # Windows

3. Instale as dependÃªncias:

pip install -r requirements.txt

## ğŸ§  Baixando e Rodando o Modelo com Ollama

1. Instale o Ollama
Acesse: https://ollama.com/download

Siga as instruÃ§Ãµes para seu sistema operacional. ApÃ³s instalar, verifique se estÃ¡ funcionando com:

ollama list

2.  Baixe um modelo (ex: LLaMA 3)

 ollama pull llama3 

3. Rode o Ollama como um servidor local (se ainda nÃ£o estiver rodando)


ollama serve

4. Altere o nome do modelo no cÃ³digo (opcional)
No main.py, o nome do modelo estÃ¡ como:

OLLAMA_MODEL = "mymodel"

Troque para:

OLLAMA_MODEL = "llama3"

Ou qualquer outro modelo que tenha sido baixado com ollama pull.

## ğŸ“‘ Etapa 1 â€“ Indexar os Documentos
Coloque seus arquivos PDF, DOCX, TXT ou CSV dentro da pasta ./data.

Depois, rode:

python  .py

## ğŸš€ Etapa 2 â€“ Iniciar a API com FastAPI
Rode o servidor backend com:

uvicorn main:app --reload

## ğŸ’¬ Etapa 3 â€“ Rodar a Interface de Chat com Streamlit
Abra outro terminal (com o ambiente virtual ativado) e execute:

streamlit run app.py

